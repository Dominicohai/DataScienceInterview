1. Describe a constrained optimization problem and how you would tackle it.
2. What are “slack variables”?
3. Do gradient descent methods always converge to same point?
4. Give examples of some convex and non-convex optimization algorithms.
5. Is it necessary that the Gradient Descent Method will always find the global minima?
6. What do you understand by statistical power of sensitivity (recall?) and how do you calculate it?
7. What is a local optimum is and why is it important in a specific context, such as k-means clustering. What are specific ways for determining if you have a local optimum problem? What can be done to avoid local optima? Read possible answer
8. What is the difference between Batch Gradient Descent and Stochastic gradient descent.
